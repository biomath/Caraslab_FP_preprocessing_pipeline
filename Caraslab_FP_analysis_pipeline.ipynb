{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "344dc9b19a3e6c21",
   "metadata": {},
   "source": [
    "# Caras lab fiber photometry analysis pipeline\n",
    "This pipeline is intended to be run after extracting processed signals with our [MatLab pipeline](https://github.com/caraslab/caraslab-fiberphotometry)\n",
    "\n",
    "Files need to be organized in a specific folder structure or file paths need to be changed\n",
    "\n",
    "File structure can be found in the Sample dataset folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adc8165366abc8a",
   "metadata": {},
   "source": [
    "## Imports and global plotting parameters\n",
    "Specific imports can be found within each function\n",
    "\n",
    "Imports do not need to be changed\n",
    "\n",
    "Change plotting parameters if desired then rerun this block"
   ]
  },
  {
   "cell_type": "code",
   "id": "30ef205fa68127e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T13:59:33.588967Z",
     "start_time": "2025-01-03T13:59:31.344586Z"
    }
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from os import remove, makedirs\n",
    "import warnings\n",
    "from platform import system\n",
    "from os.path import sep\n",
    "from re import split\n",
    "from datetime import datetime\n",
    "\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from glob import glob\n",
    "\n",
    "from matplotlib.pyplot import rcParams\n",
    "\n",
    "from helpers.run_FP_pipeline import run_pipeline\n",
    "from helpers.plot_heatmapsByAnimal import plot_heatmapsByAnimal\n",
    "from helpers.write_json import write_json\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Tweak the regex file separator for cross-platform compatibility\n",
    "if system() == 'Windows':\n",
    "    REGEX_SEP = sep * 2\n",
    "else:\n",
    "    REGEX_SEP = sep\n",
    "\n",
    "# Set plotting parameters\n",
    "label_font_size = 11\n",
    "tick_label_size = 7\n",
    "legend_font_size = 6\n",
    "line_thickness = 1\n",
    "\n",
    "rcParams['figure.dpi'] = 600\n",
    "rcParams['pdf.fonttype'] = 42\n",
    "rcParams['ps.fonttype'] = 42\n",
    "rcParams['font.family'] = 'Arial'\n",
    "rcParams['font.weight'] = 'regular'\n",
    "rcParams['axes.labelweight'] = 'regular'\n",
    "\n",
    "rcParams['font.size'] = label_font_size\n",
    "rcParams['axes.labelsize'] = label_font_size\n",
    "rcParams['axes.titlesize'] = label_font_size\n",
    "rcParams['axes.linewidth'] = line_thickness\n",
    "rcParams['legend.fontsize'] = legend_font_size\n",
    "rcParams['xtick.labelsize'] = tick_label_size\n",
    "rcParams['ytick.labelsize'] = tick_label_size\n",
    "rcParams['errorbar.capsize'] = label_font_size\n",
    "rcParams['lines.markersize'] = line_thickness\n",
    "rcParams['lines.linewidth'] = line_thickness"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "205fcc63f736d166",
   "metadata": {},
   "source": [
    "## Set global paths and variables\n",
    "\n",
    "Edit this to match your needs. \n",
    "\n",
    "This code batch analyzes folders containing recordings from multiple subjects and sessions.\n",
    "\n",
    "Organize your data in the folder structure illustrated in the Sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "id": "5f703569dd7af350",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T16:20:03.953827Z",
     "start_time": "2025-01-03T16:20:01.165520Z"
    }
   },
   "source": [
    "DATA_PATH = r'G:\\My Drive\\Documents\\PycharmProjects\\Caraslab_FP_analysis_pipeline\\Sample dataset'\n",
    "\n",
    "# Specify I/O paths and run parameters\n",
    "SETTINGS_DICT = {\n",
    "    'EXPERIMENT_TAG': 'GCaMP8s',  # Appends to start of summary files\n",
    "    \n",
    "    'EXPERIMENT_TYPE': 'AversiveAM',  # 1IFC or AversiveAM\n",
    "    \n",
    "    'SIGNALS_PATH': DATA_PATH + sep + 'Whole session signal',\n",
    "    'KEYS_PATH': DATA_PATH + sep + 'Key files',\n",
    "    'OUTPUT_PATH': DATA_PATH + sep + 'Output',\n",
    "    \n",
    "    # in seconds; for zscoring and AUC calculation\n",
    "    'BASELINE_START_FOR_ZSCORE': 0.25,  # Affects plotting and AUC calculation\n",
    "    'BASELINE_END_FOR_ZSCORE': 0.,  # Affects AUC calculation\n",
    "    'AUC_WINDOW_START': 0.,  # Affects AUC calculation\n",
    "    'AUC_WINDOW_END': 4,  # Affects AUC calculation\n",
    "    'RESPONSE_WINDOW_DURATION': 4,  # Does not affect AUC calculation; only for plotting and signal extraction\n",
    "     \n",
    "    # Ignore trials with responses shorter than this (indicative of impulsive behavior for 1IFC task).\n",
    "    # Keep at 0 for AversiveAM task or if not desired\n",
    "    'RESPONSE_LATENCY_FILTER': 0.,\n",
    "    \n",
    "    # Do you want to analyze responses aligned to trial onset or to response?\n",
    "    'TRIAL_OR_RESPONSE_ALIGNED': 'trial_aligned',  # 'trial_aligned' OR 'response_aligned'\n",
    "    \n",
    "    # Are your response values in milliseconds?\n",
    "    # This is important if you want to align by response. \n",
    "    'MS_LATENCY_VALUES': True,\n",
    "    \n",
    "    # Shock is specific to AversiveAM task and only used if you need to recalculate response latencies.\n",
    "    # Older versions of RPvds did not record latencies properly\n",
    "    'SHOCK_START_END': [0.95, 1.3],\n",
    "    \n",
    "    # Only affects plot shading but keep this in mind when calculating AUC\n",
    "    'TARGET_SOUND_ONSET': 0.,\n",
    "    'TARGET_SOUND_OFFSET': 1.,\n",
    "    \n",
    "    # For multiprocessing. Defaults to 4/5s of the number of cores\n",
    "    'NUMBER_OF_CORES': 4 * cpu_count() // 5,\n",
    "    \n",
    "    # Only run these [cells/subjects/sessions] ( has to be a list() ) or None to run all\n",
    "    # You can specify parts of the file name too or a file path with header 'Session'\n",
    "    'SESSIONS_TO_RUN': None,  \n",
    "    \n",
    "    'SESSIONS_TO_EXCLUDE': None,\n",
    "    \n",
    "    'CONCAT_SAME_DAY': True,\n",
    "    \n",
    "    'DEBUG_RUN': False,  # Turns off multiprocessing for easier debugging\n",
    "    \n",
    "    'OVERWRITE_PREVIOUS_CSV': True,  # False: appends to existing firing rate CSV file\n",
    "\n",
    "    # If None: will be estimated based on diff(Time_s) in the csv file\n",
    "    'SAMPLING_RATE': None,\n",
    "    \n",
    "    # max sampling rate for processing. Downsample if above this\n",
    "    'DOWNSAMPLE_RATE': None,  # in Hz\n",
    "    \n",
    "    'SUBTRACT_405': True,\n",
    "    \n",
    "    # These are heatmap parameters for displaying z-scores across all subjects\n",
    "    'BIN_SIZE': 0.1,  # in seconds\n",
    "        'TRIALTYPE_DICT': {\n",
    "            'Hit': [1, 2.5],\n",
    "            # 'Hit (no shock)': [0, 1.5],\n",
    "            'False alarm': [1, 2.5],\n",
    "            'Miss': [1.4, 2.9],\n",
    "            'Reject': [1.4, 2.9],\n",
    "            # 'Passive': [0, 1.5],\n",
    "        },\n",
    "    \n",
    "    'SORT_BY_WHICH_TRIALTYPE': 'Hit',\n",
    "    \n",
    "    'PLOT_PRETRIAL_DURATION': 0.5,\n",
    "    \n",
    "    'PLOT_POSTTRIAL_DURATION': 4,\n",
    "    \n",
    "    # You can set this to None and colors will be set automatically using a default colormap (max N=20 subjects for default)\n",
    "    'SUBJECT_COLORS': None,  \n",
    "    \n",
    "    # Below is a switchboard of functions you desire to run from the pipeline\n",
    "    # If you change your mind later, you can just run the ones you want and the code will add it to existing JSON files\n",
    "    'PIPELINE_SWITCHBOARD': {\n",
    "        # Only relevant to AversiveAM task when recorded using older RPvds circuit\n",
    "        'recalculate_ePsych_responseLatency': False,\n",
    "        \n",
    "        # Relevant for both AversiveAM and 1IFC tasks\n",
    "        'extract_trial_zscores': True,\n",
    "        'output_sessionData_json': True,  # Happens within extract_trial_zscores, so both have to be true for jsons to be generated\n",
    "        'plot_trial_zscores': True,\n",
    "        'plot_AMDepth_zscores': True,\n",
    "        'plot_heatmaps_by_subject': True\n",
    "    }\n",
    "}\n"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "731e823a8fb4dcf8",
   "metadata": {},
   "source": [
    "## Initial file matching then run pipelines\n",
    "\n",
    "No need to change anything here"
   ]
  },
  {
   "cell_type": "code",
   "id": "2d5a40fc433194ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T16:27:15.526587Z",
     "start_time": "2025-01-03T16:26:06.517250Z"
    }
   },
   "source": [
    "# Log the settings dictionary for reference\n",
    "write_json(SETTINGS_DICT, SETTINGS_DICT['OUTPUT_PATH'] + sep + 'Aligned signals', 'settings_dict_' + datetime.now().strftime(\"%m%d%y%H%M%S\") + '.json')\n",
    "\n",
    "# Create directory for session data json files\n",
    "makedirs(SETTINGS_DICT['OUTPUT_PATH'] + sep + 'JSON files', exist_ok=True)\n",
    "\n",
    "# Clear older multiprocessing temp files if they exist\n",
    "process_tempfiles = glob(SETTINGS_DICT['OUTPUT_PATH'] + sep + '*_tempfile_*.csv')\n",
    "[remove(f) for f in process_tempfiles]\n",
    "\n",
    "signals_path = glob(SETTINGS_DICT['SIGNALS_PATH'] + sep + '*dff.csv')\n",
    "\n",
    "if SETTINGS_DICT['SESSIONS_TO_RUN'] is not None:\n",
    "    signals_path = [path for path in signals_path if any([chosen for chosen in SETTINGS_DICT['SESSIONS_TO_RUN'] if chosen in path])]\n",
    "\n",
    "if SETTINGS_DICT['SESSIONS_TO_EXCLUDE'] is not None:\n",
    "    signals_path = [path for path in signals_path if not any([chosen for chosen in SETTINGS_DICT['SESSIONS_TO_EXCLUDE'] if chosen in path])]\n",
    "\n",
    "if len(signals_path) == 0:\n",
    "    raise UserWarning('No signal files were found. Please check your paths.')\n",
    "\n",
    "# Identify sessions from the same subject in a day if desired. Otherwise run session at a time\n",
    "subj_session_list = [split('_*_', split(REGEX_SEP, path)[-1])[0:2] for path in signals_path]\n",
    "date_list = [split('-*-', subj_session[1])[3] for subj_session in subj_session_list]\n",
    "subj_date_list = [(subj_session[0], cur_date) for subj_session, cur_date in zip(subj_session_list, date_list)]\n",
    "\n",
    "# Generate a list of inputs to be passed to each worker\n",
    "input_lists = list()\n",
    "run_list = list()\n",
    "if SETTINGS_DICT['CONCAT_SAME_DAY']:\n",
    "    # Identify sessions from the same subject in a day\n",
    "    subj_session_list = [split('_*_', split(REGEX_SEP, path)[-1])[0:2] for path in signals_path]\n",
    "    date_list = [split('-*-', subj_session[1])[2] for subj_session in subj_session_list]\n",
    "    run_list = [(subj_session[0], cur_date) for subj_session, cur_date in zip(subj_session_list, date_list)]\n",
    "else:\n",
    "    run_list = signals_path\n",
    "\n",
    "cur_date_paths_list = list()\n",
    "for dummy_idx, unique_runID in enumerate(run_list):\n",
    "    if SETTINGS_DICT['CONCAT_SAME_DAY']:\n",
    "        (unique_subj, unique_date) = unique_runID\n",
    "        subj_paths = [path for path in signals_path if unique_subj in path]\n",
    "        cur_date_paths = [path for path in subj_paths if unique_date in path]\n",
    "    else:\n",
    "        cur_date_paths = (unique_runID,)\n",
    "\n",
    "    # Avoid duplicated runs\n",
    "    if cur_date_paths in cur_date_paths_list:\n",
    "        continue\n",
    "    else:\n",
    "        cur_date_paths_list.append(cur_date_paths)\n",
    "\n",
    "for cur_date_paths in cur_date_paths_list:\n",
    "    if SETTINGS_DICT['DEBUG_RUN']:\n",
    "        run_pipeline((cur_date_paths, SETTINGS_DICT))\n",
    "    else:\n",
    "        input_lists.append((cur_date_paths, SETTINGS_DICT))\n",
    "\n",
    "if not SETTINGS_DICT['DEBUG_RUN']:\n",
    "    pool = Pool(SETTINGS_DICT['NUMBER_OF_CORES'])\n",
    "\n",
    "    # # Feed each worker with all memory paths from one unit\n",
    "    pool_map_result = pool.map(run_pipeline, input_lists)\n",
    "\n",
    "    pool.close()\n",
    "\n",
    "    pool.join()"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Extract average trial signals per session and grouping by subject in heatmaps",
   "id": "ae06d48508efd114"
  },
  {
   "cell_type": "code",
   "id": "1046dc14a4df05b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T23:12:04.968137Z",
     "start_time": "2024-11-21T23:12:01.574916Z"
    }
   },
   "source": [
    "# Summary heatmaps are not run using multiprocessing because they combine all files\n",
    "if SETTINGS_DICT['PIPELINE_SWITCHBOARD']['plot_heatmaps_by_subject']:\n",
    "    rcParams['figure.figsize'] = (4, 3)\n",
    "    plot_heatmapsByAnimal(SETTINGS_DICT)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data in CSVs...\n",
      "Uniformizing signal lengths for plotting...\n",
      "Done!\n",
      "Adding missing sessions as NaN...\n",
      "Done!\n",
      "Organizing and sorting signals for plotting...\n",
      "Done!\n",
      "Plotting...\n",
      "Done!\n"
     ]
    }
   ],
   "execution_count": 60
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
